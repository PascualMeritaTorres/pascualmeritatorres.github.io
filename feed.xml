<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://www.pascualmerita.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.pascualmerita.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-19T17:47:23+00:00</updated><id>https://www.pascualmerita.com/feed.xml</id><title type="html">blank</title><subtitle>My Personal Website.</subtitle><entry><title type="html">Where (I Think) AI Is Headed</title><link href="https://www.pascualmerita.com/blog/2025/visions_on_ai_future/" rel="alternate" type="text/html" title="Where (I Think) AI Is Headed"/><published>2025-03-05T16:40:16+00:00</published><updated>2025-03-05T16:40:16+00:00</updated><id>https://www.pascualmerita.com/blog/2025/visions_on_ai_future</id><content type="html" xml:base="https://www.pascualmerita.com/blog/2025/visions_on_ai_future/"><![CDATA[<hr/> <p>I’m writing this on March 5th, 2025, sharing my current take on where AI is headed. Like anyone trying to predict the future, I’ll probably miss the mark on some (or many) of these points - but hey, that’s part of the fun of making predictions.</p> <ul> <li> <p>The path to artificial super-intelligence likely lies in pre-trained foundation models (trained on next multimodal token prediction) that continually learn through reinforcement learning as they encounter new data and interact with the environment (this is called continual reinforcement learning).</p> </li> <li> <p>The final question we must ultimately address is: “What should self-improving AI optimise for?”</p> </li> <li> <p>In terms of self-improvement without human supervision and continual learning, we’ll solve its challenges not by entirely eliminating catastrophic forgetting in LLMs (as both LLMs and human brains have capacity limits), but by strategically forgetting less important information or compressing it into summaries to make room for new knowledge.</p> </li> <li> <p>Human work will slowly shift toward high-level tasks while AI handles (more) routine operations. The risks of completely automating menial work will drive us toward becoming a test-driven society, with systems (including AI) validate (other) AI outputs. I believe this transition is already underway for developers who use AI tools to assist with coding. For critical applications, extensive testing protocols will remain essential.</p> </li> <li> <p>The “bitter lesson” of AI development will continue: scaling simple algorithms will outperform more complex systems in the long run.</p> </li> <li> <p>Governments will recognise AI’s strategic importance (some of them are already doing so), redirecting significant portions of their budgets toward AI development, effectively initiating a new AI-focused cold war.</p> </li> <li> <p>Massive computing clusters will train and power the most advanced AI systems. While decentralised computing will exist alongside these clusters, opne-source AI will consistently lag slightly behind closed-source systems. Ultimately, due to superior computing resources, closed-source AI will pull significantly ahead. I honestly hope this does not happen but it seems like the most plausible scenario.</p> </li> <li> <p>Each of us will have a personalised AI assistant, customised to our unique data and goals.</p> </li> <li>While humans can learn through passive observation alone, they achieve optimal learning gains by alternating between observing and taking action. This same principle will be applied to AI systems, particularly embodied agents.</li> <li>Foundational model companies currently compete in an environment where users can easily switch platforms (Claude, ChatGPT, Perplexity, Gemini). The future market may shift toward tool-based subscriptions like Cursor (for coding) or Notion (for writing) that manage multiple models through a single subscription. This won’t create winner-take-all scenarios but will change how consumers access AI capabilities.</li> </ul>]]></content><author><name></name></author><category term="ai-related"/><summary type="html"><![CDATA[I’m writing this on March 5th, 2025, sharing my current take on where AI is headed. Like anyone trying to predict the future, I’ll probably miss the mark on some (or many) of these points - but hey, that’s part of the fun of making predictions.]]></summary></entry><entry><title type="html">The A* and D* Algorithms, A Programmer’s Guide to Life Decisions</title><link href="https://www.pascualmerita.com/blog/2025/life_as_a-_algo/" rel="alternate" type="text/html" title="The A* and D* Algorithms, A Programmer’s Guide to Life Decisions"/><published>2025-01-07T16:40:16+00:00</published><updated>2025-01-07T16:40:16+00:00</updated><id>https://www.pascualmerita.com/blog/2025/life_as_a*_algo</id><content type="html" xml:base="https://www.pascualmerita.com/blog/2025/life_as_a-_algo/"><![CDATA[<hr/> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">action_value</span><span class="p">(</span><span class="n">action</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="nf">hardness</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="o">+</span> <span class="nf">expected_regret</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="n">best_action</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">possible_actions</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">action_value</span><span class="p">)</span>
</code></pre></div></div> <p>Recently I was listening to a Y Combinator podcast on how to make the most out of your 20s. They all agreed that you should tackle hard problems while you’re young. In fact they all encouraged people to “do the most hardcore thing early in your career, because you can always mellow out and take on less demanding work… but it’s very hard to go the other way around”. While I fundamentally agree with this, I think there’s more to it. More specifically, you need to factor in whether these difficult challenges you’re undertaking are going to be useful in your life. In other words, you want to minimise the regret of having taken such actions.</p> <p>Anyways, this immediately reminds me of the A* algorithm.</p> <p>For a given set of actions \(n\), the A* algorithm aims to minimise the function \(f(n) = g(n) + h(n)\), where \(g(n)\) represents the distance from the start node, and \(h(n)\) the remaining estimated distance to the goal according to a certain heuristic function \(h\). Coming back to the analogy, we could think of \(-g(n)\) as how hard an action is to complete, and \(h(n)\) would be the expected regret. In other words, our aim would be to minimise the combination of how easy an action is to complete, and how much we would regret taking such action. If you prefer to read code rather than equations, I have attached a pythonesque version of the equation at the top of this blog. Just like how the A* algorithm searches for the best path through a maze, we’re searching for the best actions through life, trying to maximize personal growth (through hard actions) while minimising future regrets. If we were to follow Y Combinator’s advice, this would be equivalent to using Dijktra’s algorithm. Solely doing hard things works, but it wastes time exploring directions that aren’t worth pursuing. If we only take the regret minimisation part that I added, this would be equivalent to using Greedy Best First Search, where we are just too conservative and just perform safe actions that we are guaranteed not to regret taking.</p> <p>Coming back to the A* algorithm, here’s where it gets interesting. Unlike A*, we can’t possibly evaluate \(f(n)\) for every action available to us – there are infinitely many paths our lives could take at any moment, and we shouldn’t always sit and think for 10 minutes about what action to take next. In fact, given an infinite action space, it is theoretically impossible to evaluate the value function for all possible actions. Sometimes we need to decide quickly. Therefore, we need a tool to filter the action space in order to focus solely on the choices that actually matter. In the computer science literature, this is referred to as pruning. Pruning is a technique used to reduce search spaces by eliminating branches that are unlikely to lead to optimal solutions. A useful technique that helps people prune action branches is the Pareto Principle, also called the 80/20 rule. It states that roughly 80% of the results come from 20% of the actions. I personally don’t find thinking about this rule very useful, as evaluating whether a certain action belongs to the 20% that will lead to 80% of the results is itself an incredibly hard problem to solve. A tool which actually helps me prune actions is thinking about the opportunity cost of not taking the action. In other words, what am I giving up? I won’t expand much more on this, but I must reinforce that this doesn’t go against the “doing hard things” or “minimise regret” premises. In fact, I think it complements both. In terms of doing hard things, when you consider opportunity cost, you’re also evaluating what other challenging paths you might be giving up. In terms of regret minimisation, opportunity cost is more focused on “what else could I be doing instead?” as opposed to “will I regret doing this?”.</p> <p>Another interesting aspect about thinking of life choices algorithmically is that, unlike the traditional scenarios where A* is applied, the value function of actions change over time. Putting a value on how hard an action is and how much we will regret doing it is very subjective, and constantly shifts as we grow and change. What seems incredibly hard today might feel very natural tomorrow, and what we think we’ll regret often positively surprises us. This is where Dynamic A<em>, or D</em>, comes in. While A* assumes a static environment where all costs are known upfront, D* was specifically designed for scenarios where new information is discovered during execution, and only updates the relevant parts of the path. This is much like how we discover new opportunities and challenges as we walk through life and don’t need to completely reinvent ourselves.</p> <p>What makes A* , D* , and life decisions even more interesting is that we’re always working with approximations and incomplete information. In pathfinding algorithms, the heuristic function \(h(n)\) must be “admissible” - meaning it should never overestimate the true cost to the goal. But in life, our estimates of both hardness and regret are rarely so well-behaved. We try to make educated guesses about how difficult a career change might be, or how much we might regret not taking a certain opportunity, but these are inherently imperfect approximations. Maybe a better algorithm to use would be D* Lite (a popular variant of D*), which handles uncertainty by continuously updating its value function as new information becomes available. We too must regularly update our estimates of life’s “costs” and “benefits” as we gain experience. What we thought would definitely be a minor career pivot may turn out to be a major challenge, or what may a priori seem like a risky decision may prove to be less scary than anticipated. The key insight here is that while we can’t 100% eliminate this uncertainty - just as no pathfinding algorithm can perfectly predict unknown terrain - we can and must build systems that gracefully handle and adapt to changing environments. This is perhaps where the analogy between algorithms and life decisions is strongest: success doesn’t come from having perfect information, but from having robust methods for handling imperfect information and updating our beliefs as we go.</p>]]></content><author><name></name></author><category term="personal,"/><category term="ai-related"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Determinism, Free Will, and its relationship with AI</title><link href="https://www.pascualmerita.com/blog/2024/free_will/" rel="alternate" type="text/html" title="Determinism, Free Will, and its relationship with AI"/><published>2024-12-25T16:40:16+00:00</published><updated>2024-12-25T16:40:16+00:00</updated><id>https://www.pascualmerita.com/blog/2024/free_will</id><content type="html" xml:base="https://www.pascualmerita.com/blog/2024/free_will/"><![CDATA[<hr/> <p>Picture yourself living in a cabin deep in the forest during the final week of your 3-month spiritual retreat. Your only rule has been to maintain complete silence throughout this time, yet you feel an overwhelming urge to pick up your phone and call your loved ones. Your skin crawls, your hand hovers near the phone, and your mind races: should you break your promise and call them, or wait one more week to honor your goals? After careful consideration, you decide to call. Having spent three months in meditation, you step back and wonder what truly made you pick up the phone. Did you actually make this decision, or was this outcome inevitable? Is there such a thing as careful consideration, or consideration at all? How much does your biological state influence your choices?</p> <p>While I haven’t embarked on a 3-month forest retreat myself, I’ve thought about these questions for a while. In this blog post, I invite you to explore with me the possibility that free will doesn’t exist and how this idea suggests that we live in a purely deterministic world. We’ll examine these concepts through the lens of AI and neural networks, which, as a computer scientist, is how I approach these questions.</p> <p>Consider a neural network (in this case, a physical one called the brain) whose initial weights are determined by genetics. You can think of this as the prior in Bayesian statistics. For the first few years of life, this neural network receives input in the form of sensory experiences—sight, smell, touch, taste, and hearing—especially sight and touch. In response to these inputs, driven initially by instinct, it acts and learns from rewards. Touch fire, get burnt. Walk into a wall, get hurt. As time passes, this neural network evolves beyond pure instinct and begins generating latent tokens, which we call inner monologue. This inner monologue can take various forms: language (thinking about what to say), visual imagery (imagining an apple), or audio (a musician conceiving a melody). These latent tokens are fed back into the neural network as input, producing either more latent tokens or actions. The outcomes of these actions are also fed back into the model as a continuous stream of information.</p> <p>If you accept this description of the human brain, an interesting conclusion follows: humans have no free will. All actions are simply generated by a forward pass through a neural network, conditioned by our previous experiences, inner monologue, and genetics. Free will is an illusion…</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/free-will.drawio.svg" sizes="95vw"/> <img src="/assets/img/free-will.drawio.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. System architecture. The output tokens at timestep $$t$$ are fed as input tokens at timestep $$t+1$$, as well as all the other possible sources of input available from the environment at timestep $$t+1$$. Abstract tokens represent any form of information processing which cannot be perceived by an external observer — essentially, they represent the system's internal thought processes. </div> <p>Recently, researchers at Meta and UC San Diego implemented a very similar idea, which essentially involves <a href="https://arxiv.org/pdf/2412.06769">training large language models to reason in a continuous latent space</a>. It made me very happy to see that it improved upon vanilla chain-of-thought.</p>]]></content><author><name></name></author><category term="personal,"/><category term="ai-related"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Books</title><link href="https://www.pascualmerita.com/blog/2024/books/" rel="alternate" type="text/html" title="Books"/><published>2024-12-21T16:40:16+00:00</published><updated>2024-12-21T16:40:16+00:00</updated><id>https://www.pascualmerita.com/blog/2024/books</id><content type="html" xml:base="https://www.pascualmerita.com/blog/2024/books/"><![CDATA[<h2 id="have-read-and-recommend">Have Read (and recommend)</h2> <ul> <li> <p><a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">“Gödel, Escher, Bach: An Eternal Golden Braid”</a> by Douglas R. Hofstadter.</p> </li> <li><a href="https://en.wikipedia.org/wiki/On_the_Freedom_of_the_Will">“On the Freedom of the Will”</a>, by Arthur Schopenhauer.</li> <li> <p><a href="https://en.wikipedia.org/wiki/Nineteen_Eighty-Four">“1984”</a>, by George Orwell.</p> </li> <li> <p><a href="https://en.wikipedia.org/wiki/Animal_Farm">“Animal Farm”</a>, by George Orwell.</p> </li> <li> <p><a href="https://en.wikipedia.org/wiki/Brave_New_World">“Brave New World”</a>, by Aldous Huxley.</p> </li> <li><a href="https://en.wikipedia.org/wiki/Fahrenheit_451">“Fahrenheit 451”</a>, by Ray Bradbury.</li> </ul> <h2 id="want-to-read">Want to Read</h2> <ul> <li> <p><a href="https://en.wikipedia.org/wiki/Story_of_Your_Life">“Story of Your Life”</a>, by Ted Chiang.</p> </li> <li> <p><a href="https://en.wikipedia.org/wiki/The_Dark_Forest">“The Dark Forest”</a>, by Liu Cixin.</p> </li> <li> <p><a href="https://en.wikipedia.org/wiki/Metaphors_We_Live_By">“Metaphors We Live By”</a>, by George Lakoff and Mark Johnson.</p> </li> <li> <p><a href="https://www.danpink.com/the-power-of-regret/">“The Power of Regret: How Looking Backward Moves Us Forward”</a>, by Daniel H. Pink.</p> </li> <li> <p><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">“Thinking, Fast and Slow”</a>, by Daniel Kahneman.</p> </li> <li> <p><a href="https://en.wikipedia.org/wiki/The_Undoing_Project">“The Undoing Project”</a>, by Michael Lewis.</p> </li> <li> <p><a href="https://www.math.ias.edu/~avi/PUBLICATIONS/MYPAPERS/AW09/AW09.pdf">“Knowledge, Creativity, and P versus NP”</a>, by Avi Wigderson.</p> </li> <li> <p><a href="https://algorithmstoliveby.com/">“Algorithms to Live By”</a>, by Brian Christian and Tom Griffiths.</p> </li> <li> <p><a href="https://en.wikipedia.org/wiki/The_Pleasure_of_Finding_Things_Out">“The Pleasure of Finding Things Out”</a>, by Richard Feynman.</p> </li> <li> <p><a href="https://www.amazon.co.uk/Why-Greatness-Cannot-Planned-Objective/dp/3319155237">“Why Greatness Cannot Be Planned: The Myth of the Objective”</a>, by Kenneth O. Stanley and Joel Lehman.</p> </li> <li> <p><a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games">“Finite and Infinite Games”</a>, by James P. Carse</p> </li> </ul>]]></content><author><name></name></author><category term="personal,"/><category term="useful"/><summary type="html"><![CDATA[Have Read (and recommend)]]></summary></entry><entry><title type="html">Interesting AI Papers or Projects</title><link href="https://www.pascualmerita.com/blog/2024/interesting_ai_papers/" rel="alternate" type="text/html" title="Interesting AI Papers or Projects"/><published>2024-11-25T16:40:16+00:00</published><updated>2024-11-25T16:40:16+00:00</updated><id>https://www.pascualmerita.com/blog/2024/interesting_ai_papers</id><content type="html" xml:base="https://www.pascualmerita.com/blog/2024/interesting_ai_papers/"><![CDATA[<p>in progress.. come back in a few weeks</p> <hr/> <h5 id="multimodal">Multimodal</h5> <ul> <li><a href="https://arxiv.org/pdf/2501.07542">Imagine while Reasoning in Space: Multimodal Visualization-of-Thought</a></li> </ul> <hr/> <h5 id="vision">Vision</h5> <ul> <li><a href="https://diamond-wm.github.io/">Diffusion for World Modeling: Visual Details Matter in Atari</a></li> </ul> <hr/> <h5 id="natural-language-processing">Natural Language Processing</h5> <ul> <li> <p><a href="https://arxiv.org/pdf/2411.12580">Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models</a></p> </li> <li> <p><a href="https://arxiv.org/pdf/2403.09636">Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference</a></p> </li> <li> <p><a href="https://arxiv.org/abs/2412.06769">Training Large Language Models to Reason in a Continuous Latent Space</a></p> </li> <li> <p><a href="https://arxiv.org/pdf/2412.08821">Large Concept Models: Language Modeling in a Sentence Representation Space</a></p> </li> </ul> <hr/> <h5 id="agents">Agents</h5> <ul> <li><a href="https://github.com/SakanaAI/AI-Scientist">SakanaAI’s AI Scientist</a> and its associated <a href="https://arxiv.org/pdf/2408.06292">paper</a></li> </ul> <hr/> <h5 id="embodied-ai">Embodied AI</h5> <ul> <li><a href="https://github.com/NVIDIA/Cosmos">Cosmos: A World Development Platform</a></li> </ul>]]></content><author><name></name></author><category term="ai-related"/><summary type="html"><![CDATA[in progress.. come back in a few weeks]]></summary></entry><entry><title type="html">Small reasoner-like models vs. large knowledge-dense models</title><link href="https://www.pascualmerita.com/blog/2024/small_vs_big_models/" rel="alternate" type="text/html" title="Small reasoner-like models vs. large knowledge-dense models"/><published>2024-11-25T16:40:16+00:00</published><updated>2024-11-25T16:40:16+00:00</updated><id>https://www.pascualmerita.com/blog/2024/small_vs_big_models</id><content type="html" xml:base="https://www.pascualmerita.com/blog/2024/small_vs_big_models/"><![CDATA[<p><em>in progress.. come back in a few weeks</em></p> <hr/> <p>I am currently working on the literature review, but in the mean time, this is what I roughly want to cover:</p> <ol> <li> <p><strong>Small reasoner-like models</strong>. Motivated by Retrieval Augmented Generation (RAG), I envision a pipeline where there is an information seeking step, and a generation step. The generation step is performed by a smaller model which just focuses on reasoning over data. The two steps are coupled, creating a loop where the reasoner can ask for an extra retrieval step (indefinitely in theory). This kind of pipeline already exists, but I believe it is not yet fully exploited. I will expand more on why I believe this is the case in the article.</p> </li> <li> <p><strong>Large knowledge-dense models</strong>. Motivated by (1) the on-going research on updating the parametric knowledge of models, (2) the success of increasing the context window of LLMs, and (3) the trend of making LLMs larger and larger, I envision a pipeline where models can be constantly updated with new data (to the minute), massive amounts of data can be put in-context, and the model itself can store a lot of information in its parameters. In this pipeline, there is no need for external knowledge bases, as the model itself can act as a knowledge base and can be updated every day or multiple times a day. All the private information about a specific user/system can be put in-context.</p> </li> <li> <p><strong>Hybrid models</strong>. There are multiple ways to combine the two worlds/paradigms that I mentioned above. This is where the main body of my literature review will be focused on, as it is the part that I know the least about. An obvious way of combining it is through traditional RAG, where a (small or large) model is used to select which documents to retrieve, and then a large model is used to reason over the retrieved documents. Another (more experimental) way that I can think of is to have a sparse mixture of experts, where there is a small router model that is used to select which expert to use. Each expert can either be a small reasoner-like model or a large model. Another hybrid model that I can think of is a large model that has a reasoner-like module or system embedded in it. This is where all the recent research on reasoning is headed towards. Research such as Star, and Quiet Star (any research focused on scaling inference time compute). I will expand more on this in the article.</p> </li> </ol>]]></content><author><name></name></author><category term="ai-related"/><summary type="html"><![CDATA[in progress.. come back in a few weeks]]></summary></entry><entry><title type="html">Cool Music Performances (only live performances)</title><link href="https://www.pascualmerita.com/blog/2024/cool_music_performances/" rel="alternate" type="text/html" title="Cool Music Performances (only live performances)"/><published>2024-11-05T16:40:16+00:00</published><updated>2024-11-05T16:40:16+00:00</updated><id>https://www.pascualmerita.com/blog/2024/cool_music_performances</id><content type="html" xml:base="https://www.pascualmerita.com/blog/2024/cool_music_performances/"><![CDATA[<p>A big part of my life revolves around music. I am classicaly trained on piano by the Professional Conservatory of Music of Valencia, Spain, and I have been producing (electronic) music since the age of 14 or so. This blog serves as a display of my music taste, with the hope that I can find people with a similar taste (surprisingly this has been very difficult).</p> <p>The only requirement for this recopilation of music performances is that they are live (or pseudo-live).</p> <p>Please also note that this list is heeeavily skewed towards my 4 favourite artists: Jungle, Parcels, Jamiroquai, and Dabeull. It is also heavily skewed towards the bands that release studio quality recordings of their live performances.</p> <hr/> <ul> <li> <p><a href="https://www.youtube.com/watch?v=e8rpp-8a5s0">Jungle - Busy Earnin’, Glastonbury 2019</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=x1J6ZbKtHpw">Jungle - Busy Earnin’, Music For Life 2018</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=e4TFD2PfVPw&amp;t=1182s">Parcels - Live Vol. 1</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=H1JP759pHxI&amp;t=6110s">Parcels - Live from Red Rocks, Colorado</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=wJgRT6YDhVM&amp;list=PL1lFqScOmwRgJl_lx8o6bnvNyeAwxTHLi&amp;index=49">Parcels - Myenemy (Live @ Funkhaus Berlin)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=Gb1Z71JfI6E&amp;list=PL1lFqScOmwRgJl_lx8o6bnvNyeAwxTHLi&amp;index=45">Parcels - Hideout (Live @ Funkhaus Berlin)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=Zy4KtD98S2c">RÜFÜS DU SOL - Live from Joshua Tree</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=r4C6Kgrloxc">Anderson Paak &amp; Mac Miller- Dang! - Live</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=WZTq5do8v4s">FKJ &amp; Tom Misch - Losing My Way (Live from O2 Academy Brixton)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=pfU0QORkRpY">FKJ, Ylang Ylang EP</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=r2IoFgYV3IA&amp;list=PLJbhug9IoSL1EvNiCoj_yjAI2tHuBYJSF&amp;index=33">FKJ - Live at MELT 2019</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=hM2xWRRYA-k">FKJ - Canguu</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=M1N_wbhAfQ4">Tom Misch - It Runs Through Me</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=PmC9FsfUzy0">L’Impératrice @ La Felicità for Drop</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=SW6L_lTrIFg">C. Tangana: Tiny Desk (Home) Concert</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=pnUQYxjaWB4">Oracle Sisters - Asc. Scorpio / Tramp Like You / RBH</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=oTKz4MPd9Jw">Dabeull Band - Sweet Baby</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=Ik4DBIu8Igc">Dabeull Band - Live in Paris</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=IUJTRkrYFwo">Dabeull &amp; Rude Jude - Indastudio</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=13akCBtddRk">Dabeull - So Many Hands - Indastudio</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=gSsOJfXHk00">HER - Five Minutes - Live at JITWVHQ</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=1c1Fp8NRzrs">L’Impératrice — VACANCES (Live à Dour)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=1XNpsjpOI8A&amp;list=PLJbhug9IoSL1EvNiCoj_yjAI2tHuBYJSF&amp;index=21">Air - La Femme d’Argent</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=rlzyFxqWYaY&amp;list=PLJbhug9IoSL1EvNiCoj_yjAI2tHuBYJSF&amp;index=23">Vulfpeck - Beastly</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=qT41uNtvmmA">Jamiroquai - Virtual Insanity (Live in Verona)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=Yi0QlZZ2qo8">Jamiroquai - Little L (Live in Verona)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=ct7ad4FhuJI&amp;list=RDGMEMP-96bLtob-xyvCobnxVfyw&amp;start_radio=1&amp;rv=j8EX6i8hqqo">Jamiroquai - Time Won’t Wait (Live at Scala, London 2007)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=rMcEwaGz_64&amp;list=RDGMEMP-96bLtob-xyvCobnxVfyw&amp;index=3">Jamiroquai - Bad Girls / Singin’ in the Rain (Live in Verona)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=pRERgcQe-fQ">Nile Rodgers &amp; CHIC: Tiny Desk Concert</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=ypG3QUydnaw">Bellaire &amp; Georges - Contrasts</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=n5VRejmpMV8">Mayer Hawthorne - Henny &amp; Gingerale</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=32QLuWFfDJY&amp;list=RDGMEMJQXQAmqrnmK1SEjY_rKBGA&amp;index=4">HÆLOS - Pray (Live on KEXP)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=6sk-AWElVgM">Simply Broke - La Infinita (Mashup)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=TWxeMtklESw">Simply Broke - I wanna Be Like You (Cover)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=jtAg8_ltDEo">Michael Jackson - Don’t Stop Til You Get Enough</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=buCdGwH2Efc">Michael Jackson - Earth Song</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=NbAeffcXRe0">BB &amp; Q Band - On The Beat</a></p> </li> </ul>]]></content><author><name></name></author><category term="personal"/><summary type="html"><![CDATA[A big part of my life revolves around music. I am classicaly trained on piano by the Professional Conservatory of Music of Valencia, Spain, and I have been producing (electronic) music since the age of 14 or so. This blog serves as a display of my music taste, with the hope that I can find people with a similar taste (surprisingly this has been very difficult).]]></summary></entry><entry><title type="html">Useful Blogs (written by other people)</title><link href="https://www.pascualmerita.com/blog/2024/useful_blogs/" rel="alternate" type="text/html" title="Useful Blogs (written by other people)"/><published>2024-11-05T16:40:16+00:00</published><updated>2024-11-05T16:40:16+00:00</updated><id>https://www.pascualmerita.com/blog/2024/useful_blogs</id><content type="html" xml:base="https://www.pascualmerita.com/blog/2024/useful_blogs/"><![CDATA[<p>This is a carefully curated (and still incomplete) collection of blogposts which in my opinion are worth a read. I’m positively surprised by the sheer amount of influential (and not so influential) people that spend a lot of their free time writing down their thoughts and learnings. Enjoy!</p> <hr/> <h2 id="ai-related-technical">AI-related (Technical)</h2> <h4 id="classics">Classics</h4> <ul> <li> <p>Richard Sutton’s <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a>. My biggest takeaway from it is that</p> <blockquote> <p>“building how we think we think does not work in the long run”</p> </blockquote> </li> </ul> <h4 id="non-classics-yet">Non-Classics (Yet)?</h4> <ul> <li> <p>Evan Miller´s <a href="https://www.evanmiller.org/attention-is-off-by-one.html">Attention is Off By One</a></p> </li> <li> <p>Aidan McLaughlin’s <a href="https://aidanmclaughlin.notion.site/reasoners-problem">The Problem with (LLM) Reasoners</a></p> </li> <li> <p>Yi Tay’s <a href="https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising">What happened to BERT &amp; T5? On Transformer Encoders, PrefixLM and Denoising Objectives</a></p> </li> <li> <p>Kevin Liu’s <a href="https://kliu.io/post/llms-can-simulate-everything/">Large Language Models can Simulate Everything</a></p> </li> </ul> <hr/> <h2 id="ai-related-non-technical">AI-related (Non-Technical)</h2> <ul> <li> <p>Tim Rocktäschel’s <a href="https://rockt.github.io/2018/08/29/msc-advice">Advice for Short-term Machine Learning Research Projects</a></p> </li> <li> <p>Mor Harchol-Balter’s <a href="https://www.cs.cmu.edu/~harchol/gradschooltalk.pdf">Applying to Ph.D. Programs in CS</a></p> </li> <li> <p>Andrej Karpathy’s <a href="https://karpathy.github.io/2016/09/07/phd/">A Survival Guide to a PhD</a>, which in my opinion is completely generalisable to doing research in general, not only during a PhD. In fact, I think some of his thoughts (such as developing taste) are applicable to working in almost any job in general.</p> </li> <li> <p>Jason Eisner’s <a href="https://www.cs.jhu.edu/~jason/advice/how-to-read-a-paper.html">How to Read a Paper</a></p> </li> <li> <p>Jason Eisner’s <a href="https://www.cs.jhu.edu/~jason/advice/write-the-paper-first.html?ref=ruder.io">Write the Paper First</a></p> <blockquote> <p>There are 2 reasons a paper will be cited. 1) If you have a great implementation that people can just use as a black box. 2) Otherwrise, your paper is only useful for the ideas that it provides.</p> </blockquote> </li> <li> <p>Sebastian Ruder’s <a href="https://www.ruder.io/10-tips-for-research-and-a-phd/#1-read-broadly-">10 Tips for Research and a PhD</a></p> </li> <li> <p>Richard Hamming’s <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html?ref=ruder.io">You and Your Research</a></p> </li> <li> <p>Jakob Foerster’s <a href="https://docs.google.com/document/d/16R1E2ExKUCP5SlXWHr-KzbVDx9DBUclra-EbU8IB-iE/edit?tab=t.0#heading=h.16t67gkeu9dx">How to ML Paper</a></p> </li> <li> <p>Tim Dettmers’s <a href="https://timdettmers.com/2018/11/26/phd-applications/">Machine Learning PhD Applications - Everything You Need to Know</a></p> </li> </ul> <h2 id="personal-life">Personal Life</h2> <ul> <li> <p>Spruce Campbell’s <a href="https://spruce.world/blog/dont-ask-successful-people-for-advice.html">Don´t Ask Successful People for Advice</a>, instead ask for criticism.</p> <blockquote> <p>(If you ask for criticism, ) what they say to you is probably going to be much more actionable, direct and sensible than if you asked for generic advice.</p> </blockquote> </li> <li> <p>Spruce Campbell’s <a href="https://spruce.world/blog/talking-to-geniuses.html">Talking to Geniuses</a></p> </li> <li> <p>Paul Graham’s <a href="https://www.paulgraham.com/persistence.html#f1n">The Right Kind of Stubborn</a></p> <blockquote> <p>The persistent (good kind of stubborn) are attached to the goal. The obstinate (bad kind of stubborn) are attached to their ideas about how to reach it.</p> </blockquote> </li> <li> <p>Sam Altman’s <a href="https://blog.samaltman.com/how-to-be-successful">How to Be Successful</a></p> <blockquote> <p>It is much more important to work on the right thing than it is to work many hours.</p> </blockquote> </li> <li> <p>Paul Graham’s <a href="https://paulgraham.com/taste.html">Taste for Makers</a></p> <blockquote> <p>When you’re forced to be simple, you’re forced to face the real problem. When you can’t deliver ornament, you have to deliver substance.</p> </blockquote> </li> <li> <p><a href="https://www.washingtonpost.com/news/wonk/wp/2016/02/16/when-to-stop-dating-and-settle-down-according-to-math/">‘The Secretary Problem’</a></p> </li> <li> <p>Robert Wiblin’s <a href="https://medium.com/@robertwiblin/the-secretary-problem-is-too-bad-a-match-for-real-life-to-usefully-inform-our-decisions-so-1cd29ae01024">‘The Secretary Problem’ is too bad a match for real life to usefully inform our decisions</a></p> </li> <li> <p>Kevin Liu’s <a href="https://kliu.io/post/searching-for-community/">How do you find your people, when searching is itself an antipattern?</a></p> </li> <li> <p>Sam Altman’s <a href="https://blog.samaltman.com/advice-for-ambitious-19-year-olds">Advice for ambitious 19 year olds</a></p> </li> </ul>]]></content><author><name></name></author><category term="useful"/><summary type="html"><![CDATA[This is a carefully curated (and still incomplete) collection of blogposts which in my opinion are worth a read. I’m positively surprised by the sheer amount of influential (and not so influential) people that spend a lot of their free time writing down their thoughts and learnings. Enjoy!]]></summary></entry><entry><title type="html">Useful Podcasts / Videos</title><link href="https://www.pascualmerita.com/blog/2024/useful_podcasts/" rel="alternate" type="text/html" title="Useful Podcasts / Videos"/><published>2024-11-05T16:40:16+00:00</published><updated>2024-11-05T16:40:16+00:00</updated><id>https://www.pascualmerita.com/blog/2024/useful_podcasts</id><content type="html" xml:base="https://www.pascualmerita.com/blog/2024/useful_podcasts/"><![CDATA[<p>Some (or I should probably say a lot) of the content that I consume on a daily basis is in the form of podcasts or youtube videos in general. This is an incomplete collection of all of the ones that might be useful to someone with a similar background to me.</p> <hr/> <h2 id="ai-related">AI-related</h2> <h4 id="classics">Classics</h4> <ul> <li> <p><a href="https://www.youtube.com/watch?v=13CZPWmke6A">Ilya Sutskever</a> on the Lex Fridman Podcast</p> </li> <li> <p><a href="https://www.youtube.com/watch?v=cdiD-9MMpb0&amp;t=6209s">Andrej Karpathy</a> on the Lex Fridman Podcast</p> </li> </ul> <h4 id="non-classics-yet">Non-Classics (Yet)?</h4> <ul> <li> <p><a href="https://www.youtube.com/watch?v=ugvHCXCOmm4&amp;t=1069s">Dario Amoidei</a> on the Lex Fridman Podcast</p> </li> <li> <p>Sasha Rush’s <a href="https://www.youtube.com/watch?v=6PEJ96k1kiw&amp;t=593s">Speculations on Test-Time Scaling (o1)</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=oFfVt3S51T4&amp;t=7426s">Cursor team</a> on the Lex Fridman Podcast</p> </li> <li> <p><a href="https://www.youtube.com/watch?v=WXuK6gekU1Y">AlphaGo Documentary</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=rgP_LBtaUEc">PyTorch Documentary</a></p> </li> <li> <p><a href="https://www.youtube.com/watch?v=Yf1o0TQzry8">Ilya Sutskever</a> on Dwarkesh Patel’s Podcast</p> </li> <li> <p><a href="https://www.youtube.com/watch?v=n4IQOBka8bc">Geoffrey Hinton</a> on Sana’s channel</p> </li> <li> <p>Noam Brown’s lecture on <a href="https://www.youtube.com/watch?v=Gr_eYXdHFis"> Learning to Reason with LLMs</a></p> </li> </ul>]]></content><author><name></name></author><category term="useful"/><summary type="html"><![CDATA[Some (or I should probably say a lot) of the content that I consume on a daily basis is in the form of podcasts or youtube videos in general. This is an incomplete collection of all of the ones that might be useful to someone with a similar background to me.]]></summary></entry></feed>